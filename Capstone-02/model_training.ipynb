{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f96c7af",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Model Training Pipeline\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline for heart disease prediction, including:\n",
    "- Data loading and exploration\n",
    "- Preprocessing and feature engineering\n",
    "- Training multiple classification models\n",
    "- Hyperparameter tuning\n",
    "- Model evaluation and comparison\n",
    "- Saving the best model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d5d22",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Evaluation\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix, \n",
    "                            classification_report, roc_curve, auc)\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972d04b",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('heart_disease_dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d675fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419448a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nClass Balance: {df['target'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "df['target'].value_counts().plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Target Variable Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Target (0=No Disease, 1=Disease)', fontsize=11)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].set_xticklabels(['No Disease (0)', 'Disease (1)'], rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "target_counts = df['target'].value_counts()\n",
    "axes[1].pie(target_counts, labels=['Disease (1)', 'No Disease (0)'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=['#e74c3c', '#3498db'],\n",
    "           explode=(0.05, 0))\n",
    "axes[1].set_title('Target Variable Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  No Disease (0): {target_counts[0]} ({target_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"  Disease (1): {target_counts[1]} ({target_counts[1]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution and correlation matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Target distribution\n",
    "axes[0].pie(df['target'].value_counts(), labels=['No Disease (0)', 'Disease (1)'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Correlation heatmap\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "           linewidths=0.5, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
    "axes[1].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff469c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature types\n",
    "continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "print(\"Continuous Features:\", continuous_features)\n",
    "print(\"\\nCategorical Features:\", categorical_features)\n",
    "print(\"\\nUnique values in categorical features:\")\n",
    "for col in categorical_features:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values - {sorted(df[col].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features distribution\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    df[col].value_counts().sort_index().plot(kind='bar', ax=axes[idx], color='steelblue')\n",
    "    axes[idx].set_title(f'{col.upper()} Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize continuous features distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(continuous_features):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[idx], color='coral', bins=30)\n",
    "    axes[idx].set_title(f'{col.upper()} Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_val = df[col].mean()\n",
    "    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.1f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by target\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[df['target'] == 0]['age'].hist(bins=20, alpha=0.7, label='No Disease', color='blue', edgecolor='black')\n",
    "df[df['target'] == 1]['age'].hist(bins=20, alpha=0.7, label='Disease', color='red', edgecolor='black')\n",
    "plt.xlabel('Age', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Age Distribution by Heart Disease', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.boxplot(column='age', by='target', ax=plt.gca(), patch_artist=True)\n",
    "plt.xlabel('Target (0=No Disease, 1=Disease)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Age', fontsize=11, fontweight='bold')\n",
    "plt.title('Age Distribution by Target', fontsize=13, fontweight='bold')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed89202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for skewness in continuous features\n",
    "print(\"=\" * 60)\n",
    "print(\"SKEWNESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "skewness = df[continuous_features].skew()\n",
    "for feature, skew_value in skewness.items():\n",
    "    skew_type = \"Highly Skewed\" if abs(skew_value) > 1 else \"Moderately Skewed\" if abs(skew_value) > 0.5 else \"Approximately Normal\"\n",
    "    print(f\"{feature:15} : {skew_value:6.3f}  ({skew_type})\")\n",
    "\n",
    "# Visualize skewness\n",
    "plt.figure(figsize=(10, 5))\n",
    "skewness.plot(kind='bar', color='teal', edgecolor='black')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=1, color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "plt.axhline(y=-1, color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "plt.title('Skewness of Continuous Features', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=11)\n",
    "plt.ylabel('Skewness Value', fontsize=11)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using boxplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(continuous_features):\n",
    "    df.boxplot(column=col, ax=axes[idx], patch_artist=True, \n",
    "              boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'{col.upper()} - Outlier Detection', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calculate IQR and outliers\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)][col]\n",
    "    axes[idx].text(0.5, 0.95, f'Outliers: {len(outliers)}', \n",
    "                  transform=axes[idx].transAxes, ha='center', va='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with target\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlations = df.corr()['target'].drop('target').sort_values(ascending=False)\n",
    "colors = ['green' if x > 0 else 'red' for x in correlations]\n",
    "correlations.plot(kind='barh', color=colors, edgecolor='black')\n",
    "plt.title('Feature Correlation with Target Variable', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=11)\n",
    "plt.ylabel('Features', fontsize=11)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Positively Correlated Features:\")\n",
    "print(correlations.head())\n",
    "print(\"\\nTop 5 Negatively Correlated Features:\")\n",
    "print(correlations.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9eb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for key continuous features\n",
    "key_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'target']\n",
    "sns.pairplot(df[key_features], hue='target', palette={0: 'blue', 1: 'red'}, \n",
    "            diag_kind='kde', plot_kws={'alpha': 0.6}, height=2.5)\n",
    "plt.suptitle('Pairplot of Continuous Features by Target', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954111b",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nFeature columns:\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d81dcf",
   "metadata": {},
   "source": [
    "## 4. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% train, 20% test) with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n",
    "print(\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTesting set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for better readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"✓ Features scaled successfully!\")\n",
    "print(\"\\nScaled training data sample:\")\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbbce11",
   "metadata": {},
   "source": [
    "## 5. Initialize and Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed571948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'SVM': SVC(random_state=RANDOM_STATE, probability=True)\n",
    "}\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  • {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and store results\n",
    "trained_models = {}\n",
    "results = []\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  ✓ {name} trained - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f if roc_auc else 'N/A'}\")\n",
    "\n",
    "print(\"\\n✓ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334d04e",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_xlabel('Accuracy', fontweight='bold')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "\n",
    "# Plot 2: ROC-AUC comparison\n",
    "axes[0, 1].barh(results_df['Model'], results_df['ROC-AUC'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('ROC-AUC Score', fontweight='bold')\n",
    "axes[0, 1].set_title('Model ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlim([0, 1])\n",
    "\n",
    "# Plot 3: Precision, Recall, F1-Score\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(results_df['Model']))\n",
    "width = 0.25\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    axes[1, 0].bar(x + i*width, results_df[metric], width, label=metric)\n",
    "axes[1, 0].set_xlabel('Models', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Score', fontweight='bold')\n",
    "axes[1, 0].set_title('Precision, Recall, and F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x + width)\n",
    "axes[1, 0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 4: Overall metrics heatmap\n",
    "metrics_heatmap = results_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "sns.heatmap(metrics_heatmap, annot=True, fmt='.3f', cmap='YlGnBu', ax=axes[1, 1], \n",
    "           cbar_kws={'label': 'Score'}, vmin=0, vmax=1)\n",
    "axes[1, 1].set_title('Performance Metrics Heatmap', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a47c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "               xticklabels=['No Disease', 'Disease'],\n",
    "               yticklabels=['No Disease', 'Disease'])\n",
    "    axes[idx].set_title(f'{name} - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda24ceb",
   "metadata": {},
   "source": [
    "## 7. Tune Hyperparameters (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c031aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model from baseline results (typically Random Forest)\n",
    "best_baseline_model = results_df.iloc[0]['Model']\n",
    "print(f\"Best baseline model: {best_baseline_model}\")\n",
    "print(f\"Baseline ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")\n",
    "print(\"\\nPerforming hyperparameter tuning on Random Forest...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Testing {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['max_features']) * len(param_grid['bootstrap'])} parameter combinations...\")\n",
    "print(\"This may take a few minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc72f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Grid Search\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n✓ Grid Search completed in {elapsed_time:.2f} seconds!\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation ROC-AUC score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_tuned = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics for tuned model\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "tuned_precision = precision_score(y_test, y_pred_tuned)\n",
    "tuned_recall = recall_score(y_test, y_pred_tuned)\n",
    "tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "tuned_roc_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TUNED MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {tuned_accuracy:.4f}\")\n",
    "print(f\"Precision: {tuned_precision:.4f}\")\n",
    "print(f\"Recall:    {tuned_recall:.4f}\")\n",
    "print(f\"F1-Score:  {tuned_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {tuned_roc_auc:.4f}\")\n",
    "print(\"\\n\" + classification_report(y_test, y_pred_tuned, target_names=['No Disease', 'Disease']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs tuned model\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline Random Forest', 'Tuned Random Forest'],\n",
    "    'Accuracy': [results_df[results_df['Model'] == 'Random Forest']['Accuracy'].values[0], tuned_accuracy],\n",
    "    'Precision': [results_df[results_df['Model'] == 'Random Forest']['Precision'].values[0], tuned_precision],\n",
    "    'Recall': [results_df[results_df['Model'] == 'Random Forest']['Recall'].values[0], tuned_recall],\n",
    "    'F1-Score': [results_df[results_df['Model'] == 'Random Forest']['F1-Score'].values[0], tuned_f1],\n",
    "    'ROC-AUC': [results_df[results_df['Model'] == 'Random Forest']['ROC-AUC'].values[0], tuned_roc_auc]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE vs TUNED MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb766276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance from tuned model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='steelblue')\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.title('Feature Importance - Tuned Random Forest Model', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head().iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance comparison visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "results_df.plot(x='Model', y=['Accuracy'], kind='bar', ax=ax1, color='skyblue', legend=False)\n",
    "ax1.set_title('Model Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. ROC-AUC Comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "results_df.plot(x='Model', y=['ROC-AUC'], kind='bar', ax=ax2, color='lightcoral', legend=False)\n",
    "ax2.set_title('ROC-AUC Score Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('ROC-AUC', fontweight='bold')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Precision-Recall-F1 Comparison\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "ax3.bar(x_pos - width, results_df['Precision'], width, label='Precision', color='#3498db')\n",
    "ax3.bar(x_pos, results_df['Recall'], width, label='Recall', color='#e74c3c')\n",
    "ax3.bar(x_pos + width, results_df['F1-Score'], width, label='F1-Score', color='#2ecc71')\n",
    "ax3.set_title('Precision, Recall, F1-Score', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Score', fontweight='bold')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax3.set_ylim([0, 1])\n",
    "ax3.legend(loc='lower right')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Heatmap of all metrics\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "metrics_heatmap = results_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "sns.heatmap(metrics_heatmap.T, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax4, \n",
    "           cbar_kws={'label': 'Score'}, vmin=0, vmax=1, linewidths=0.5)\n",
    "ax4.set_title('Performance Metrics Heatmap (All Models)', fontsize=13, fontweight='bold')\n",
    "ax4.set_xlabel('Models', fontweight='bold')\n",
    "ax4.set_ylabel('Metrics', fontweight='bold')\n",
    "\n",
    "# 5. Best model ranking\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "results_sorted = results_df.sort_values('ROC-AUC', ascending=True)\n",
    "colors_rank = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(results_sorted)))\n",
    "ax5.barh(results_sorted['Model'], results_sorted['ROC-AUC'], color=colors_rank, edgecolor='black')\n",
    "ax5.set_title('Model Ranking by ROC-AUC Score', fontsize=13, fontweight='bold')\n",
    "ax5.set_xlabel('ROC-AUC Score', fontweight='bold')\n",
    "ax5.set_xlim([0, 1])\n",
    "ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for idx, (model, score) in enumerate(zip(results_sorted['Model'], results_sorted['ROC-AUC'])):\n",
    "    ax5.text(score + 0.01, idx, f'{score:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comprehensive Model Performance Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a643f",
   "metadata": {},
   "source": [
    "## 8. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = 'models/best_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"✓ Best model saved to: {model_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = 'data/processed/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'test_metrics': {\n",
    "        'accuracy': tuned_accuracy,\n",
    "        'precision': tuned_precision,\n",
    "        'recall': tuned_recall,\n",
    "        'f1_score': tuned_f1,\n",
    "        'roc_auc': tuned_roc_auc\n",
    "    },\n",
    "    'feature_names': list(X.columns),\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "metadata_path = 'models/model_metadata.pkl'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"✓ Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model file: {model_path}\")\n",
    "print(f\"Scaler file: {scaler_path}\")\n",
    "print(f\"Metadata file: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b264591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "print(\"Testing model loading...\")\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "\n",
    "# Make a test prediction\n",
    "test_sample = X_test.iloc[0:1]\n",
    "test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "test_prediction = loaded_model.predict(test_sample_scaled)\n",
    "test_probability = loaded_model.predict_proba(test_sample_scaled)[:, 1]\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "print(f\"\\nTest prediction:\")\n",
    "print(f\"  Input features: {test_sample.values[0][:5]}... (first 5 features)\")\n",
    "print(f\"  Prediction: {'Disease' if test_prediction[0] == 1 else 'No Disease'}\")\n",
    "print(f\"  Probability: {test_probability[0]:.4f}\")\n",
    "print(f\"  Actual: {'Disease' if y_test.iloc[0] == 1 else 'No Disease'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31897488",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Dataset**: Heart disease prediction with 400 patients and 13 features\n",
    "2. **Models Evaluated**: Decision Tree, Random Forest, Logistic Regression, SVM\n",
    "3. **Best Model**: Random Forest (after hyperparameter tuning)\n",
    "4. **Performance**: Achieved ROC-AUC score on test set\n",
    "5. **Most Important Features**: Identified through feature importance analysis\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy the model using the FastAPI backend (api/main.py)\n",
    "- Test the model through the web interface (static/index.html)\n",
    "- Monitor model performance in production\n",
    "- Consider collecting more data for improved accuracy\n",
    "- Implement model versioning and A/B testing\n",
    "\n",
    "### Files Generated:\n",
    "- `models/best_model.pkl` - Trained Random Forest model\n",
    "- `data/processed/scaler.pkl` - StandardScaler for feature normalization\n",
    "- `models/model_metadata.pkl` - Model parameters and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive project summary\n",
    "print(\"=\" * 80)\n",
    "print(\"HEART DISEASE PREDICTION - PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n📊 DATASET STATISTICS:\")\n",
    "print(f\"   • Total Records: {df.shape[0]}\")\n",
    "print(f\"   • Total Features: {df.shape[1] - 1}\")\n",
    "print(f\"   • Continuous Features: {len(continuous_features)}\")\n",
    "print(f\"   • Categorical Features: {len(categorical_features)}\")\n",
    "print(f\"   • Missing Values: {df.isnull().sum().sum()}\")\n",
    "print(f\"   • Class Distribution: {dict(df['target'].value_counts())}\")\n",
    "\n",
    "print(f\"\\n🔬 DATA SPLIT:\")\n",
    "print(f\"   • Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"   • Testing Samples: {X_test.shape[0]}\")\n",
    "print(f\"   • Train/Test Ratio: {X_train.shape[0]/X_test.shape[0]:.1f}:1\")\n",
    "\n",
    "print(f\"\\n🤖 MODELS EVALUATED:\")\n",
    "for idx, model_name in enumerate(results_df['Model'], 1):\n",
    "    print(f\"   {idx}. {model_name}\")\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL:\")\n",
    "best_idx = results_df['ROC-AUC'].idxmax()\n",
    "best_model_name = results_df.loc[best_idx, 'Model']\n",
    "print(f\"   • Model: {best_model_name}\")\n",
    "print(f\"   • Test Accuracy: {results_df.loc[best_idx, 'Accuracy']:.4f}\")\n",
    "print(f\"   • Test Precision: {results_df.loc[best_idx, 'Precision']:.4f}\")\n",
    "print(f\"   • Test Recall: {results_df.loc[best_idx, 'Recall']:.4f}\")\n",
    "print(f\"   • Test F1-Score: {results_df.loc[best_idx, 'F1-Score']:.4f}\")\n",
    "print(f\"   • ROC-AUC Score: {results_df.loc[best_idx, 'ROC-AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 HYPERPARAMETER TUNING:\")\n",
    "print(f\"   • Best Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"      - {param}: {value}\")\n",
    "print(f\"   • CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 SAVED ARTIFACTS:\")\n",
    "print(f\"   • Best Model: models/best_model.pkl\")\n",
    "print(f\"   • Scaler: data/processed/scaler.pkl\")\n",
    "print(f\"   • Model Metadata: models/model_metadata.pkl\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(f\"   • Top 3 Important Features:\")\n",
    "for idx, (feat, imp) in enumerate(feature_importance.head(3).values, 1):\n",
    "    print(f\"      {idx}. {feat}: {imp:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ PROJECT STATUS: COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0b6f1",
   "metadata": {},
   "source": [
    "## Environment Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bec498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Python and library versions\n",
    "import sys\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPython Version: {sys.version.split()[0]}\")\n",
    "print(f\"\\nLibrary Versions:\")\n",
    "print(f\"  • pandas:      {pd.__version__}\")\n",
    "print(f\"  • numpy:       {np.__version__}\")\n",
    "print(f\"  • matplotlib:  {plt.matplotlib.__version__}\")\n",
    "print(f\"  • seaborn:     {sns.__version__}\")\n",
    "print(f\"  • scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"  • scipy:       {scipy.__version__}\")\n",
    "print(f\"  • joblib:      {joblib.__version__}\")\n",
    "\n",
    "print(f\"\\nWorkspace: {os.getcwd()}\")\n",
    "print(f\"Random State: {RANDOM_STATE}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
